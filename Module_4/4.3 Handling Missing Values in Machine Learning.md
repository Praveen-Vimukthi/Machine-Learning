# üìä Handling Missing Values in Machine Learning | Imputation | Dropping

This project demonstrates strategies for dealing with missing values in a dataset, using Python and pandas. Missing data can significantly affect the performance of machine learning models, and it's crucial to preprocess such data appropriately.

---

## üîç What Are Missing Values?

Missing values (often shown as `NaN` in pandas) can occur due to:

- Incomplete data collection
- Errors during data entry
- Sensor or system failures

Many ML models cannot handle `NaN` values, so preprocessing is required.

---

## üß™ Practical Implementation in Python

This demonstrates how to handle missing data using Python's pandas library:
- **Detecting Missing Values**: Use functions like isnull() or info() to identify missing entries in the dataset.
- **Dropping Data**: Apply dropna() to remove rows or columns with missing values.
- **Imputing Data**: Utilize fillna() to replace missing values with specified statistics or use scikit-learn's SimpleImputer for more advanced imputation strategies.

---

## üßπ Strategies to Handle Missing Values

### 1. Dropping Data

- **Drop Rows**: Use when only a few records have missing values.
```python
df.dropna(inplace=True) # drops rows with any missing values
```

- **Drop Columns**: Use when a column has too many missing values (e.g., >50%) and isn‚Äôt critical.
```python
df.dropna(axis=1, inplace=True)  # drops columns with missing values
```

---

### 2. Imputation

üìå What is Imputation?

**Imputation** is the process of replacing missing data with substituted values. This helps retain all rows in a dataset and avoids loss of information due to dropping rows or columns.


üìå What are Outliers?

An **outlier** is a data point that is noticeably different from the other data points. It is often much **larger** or **smaller** than most of the values in the dataset.

‚úÖ Example of an Outlier:

Consider the following test scores:

scores = [85, 87, 90, 88, 95, 92, 20]

Most scores are in the 85‚Äì95 range, but 20 is way off.
‚û°Ô∏è So, 20 is an outlier.


‚úÖ When to Use Mean, Median, or Mode Imputation

| Method | Best For | Example Feature | Description | Code Example |
|--------|----------|-----------------|-------------|--------------|
| **Mean** | Normally distributed numeric data | `Age` | Replaces missing values with the average | `df['Age'].fillna(df['Age'].mean(), inplace=True)` |
| **Median** | Skewed data or when outliers are present | `Income` | Uses the middle value of the sorted column | `df['Income'].fillna(df['Income'].median(), inplace=True)` |
| **Mode** | Categorical or discrete data | `Gender` | Replaces with the most frequent category | `df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)` |

üß™ Code Example

```python
import pandas as pd

# Sample dataset
data = {
    'Age': [25, 30, None, 28],
    'Income': [3000, 3500, None, 100000],
    'Gender': ['Male', None, 'Female', 'Male']
}

df = pd.DataFrame(data)

# Imputation
df['Age'].fillna(df['Age'].mean(), inplace=True)           # Mean for Age
df['Income'].fillna(df['Income'].median(), inplace=True)   # Median for Income
df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)  # Mode for Gender

print(df)
```

ü§ñ Advanced Imputation with SimpleImputer (scikit-learn)

```python
from sklearn.impute import SimpleImputer

# Numerical imputation with mean
num_imputer = SimpleImputer(strategy='mean')
df[['Age', 'Income']] = num_imputer.fit_transform(df[['Age', 'Income']])

# Categorical imputation with most frequent (mode)
cat_imputer = SimpleImputer(strategy='most_frequent')
df[['Gender']] = cat_imputer.fit_transform(df[['Gender']])
```

```
     Age   Income Gender
0  25.00   3000.0   Male
1  30.00   3500.0   Male
2  27.67   3500.0 Female
3  28.00 100000.0   Male
```

---

## üß† Choosing the Right Approach

The decision between dropping and imputing missing data depends on factors such as the proportion of missingness, the importance of the affected features, and the assumptions of the machine learning models being used. Careful consideration ensures that the handling of missing data does not introduce bias or reduce the model's predictive performance.
