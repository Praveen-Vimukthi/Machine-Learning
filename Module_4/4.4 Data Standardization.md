# Standardization in Machine Learning

## ğŸ“Œ Overview

In Machine Learning, **data preprocessing** is crucial for model performance.  
**Feature Scaling** ensures numeric features are on a similar scale.  

This repository focuses on **Standardization**, a method to scale features.

---

## ğŸ“ What is Standardization?

Standardization rescales numeric features so they have:

- **Mean (Î¼) = 0**
- **Standard Deviation (Ïƒ) = 1**

It helps models converge faster and perform better when features have vastly different ranges.

Mathematically, it is represented by the **Z-Score formula:**: 

$$
Z = \frac{X - \mu}{\sigma}
$$

Where:

- $$X\$$ = original value  
- $$\mu$$ = mean of dataset  
- &sigma;  = standard deviation  

**Key Points:**

- Values roughly lie between **-1 and 1**.  
- Maintains relative differences between values.  
- Improves training for models like **Gradient Descent-based models**.

---

## ğŸ“Š Example Dataset
We will use this dataset:

```python
dataset_1 = [1, 99, 789, 5, 6859, 541, 94142, 7, 50826, 35464]
dataset_0 = [10, 5, 6, 1, 3, 7, 9, 4, 8, 2]  # x-axis
```

### ğŸ“ˆ Step 1 â€” Compute Mean and Standard Deviation
```python
n = len(dataset_1)
mean = sum(dataset_1) / n
variance = sum([(x - mean)**2 for x in dataset_1]) / n
std_dev = variance**0.5

print("Mean: ", mean)
print("Standard Deviation: ", std_dev)
```
**Output:**
```
Mean: 18873.3
Standard Deviation: 1275166469.5
```

### ğŸ“ Step 2 â€” Standardize (Z-Score)
```python
# Calculating the Z-Score for each 
# Value of dataset_1
final_z_score = []
print("Calculating Z-Score of Each Value in dataset_1")

for i in dataset_1:
    z_score = (i-mean)/standard_deviation
    final_z_score.append("{:.20f}".format(z_score))

# Comparing the Values of Original Dataset and Saled Down Dataset
print("\nOriginal DataSet   |               Z-Score ")
print()
for i in range(len(dataset_1)):
    print("    ", dataset_1[i], "          |     ", final_z_score[i])
```
**Output:**

| Original DataSet | Z-Score                 |
| ---------------- | ----------------------- |
| 1                | -0.00001479987158649171 |
| 99               | -0.00001472301887561513 |
| 789              | -0.00001418191305413712 |
| 5                | -0.00001479673474114981 |
| 6859             | -0.00000942175024780167 |
| 541              | -0.00001437639746533501 |
| 94142            | 0.00005902656774649453  |
| 7                | -0.00001479516631847886 |
| 50826            | 0.00002505766953904366  |
| 35464            | 0.00001301061500347112  |

Now we will compare and see the graph of the Original Values and the Standardized Values

### ğŸ“Š Step 3 â€” Plot the Original Data
```python
import matplotlib.pyplot as plt

# Here We are checking the Graph
# of the Original Values
plt.scatter(dataset_0, dataset_1, label="stars",
            color="blue", marker="*", s=40)
plt.xlabel('x - axis')
plt.ylabel('y - axis')
plt.legend()
plt.show()
```
**Output:**

<img src="https://media.geeksforgeeks.org/wp-content/uploads/20220822123457/1.PNG"/>

### ğŸ“‰ Step 4 â€” Plot Standardized Values
```python
import matplotlib.pyplot as plt

# Here we are checking the Graph of
# the Standardized Values
plt.scatter(dataset_0, final_z_score, label="stars",
            color="blue", marker="*", s=30)
plt.xlabel('x - axis')
plt.ylabel('y - axis')
plt.legend()
plt.show()
```
**Output:**

<img src="https://media.geeksforgeeks.org/wp-content/uploads/20220822123458/2.PNG"/>

---

## ğŸ¤” Why Standardize?

- Faster convergence in training
- Prevents large features from dominating
- Helps distance-based models like KNN and SVM
- Normalizes spread of values
  
---
## ğŸ›  Using Scikitâ€‘learn
```python
from sklearn.preprocessing import StandardScaler
import numpy as np

dataset_np = np.array(dataset_1).reshape(-1, 1)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(dataset_np)

print(scaled_data)
```

