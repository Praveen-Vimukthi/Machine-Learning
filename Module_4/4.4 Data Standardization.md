# Standardization in Machine Learning

## ğŸ“Œ Overview

In Machine Learning, **data preprocessing** is crucial for model performance.  
**Feature Scaling** ensures numeric features are on a similar scale.  

This repository focuses on **Standardization**, a method to scale features.

---

## ğŸ“ What is Standardization?

Standardization rescales numeric features so they have:

- **Mean (Î¼) = 0**
- **Standard Deviation (Ïƒ) = 1**

It helps models converge faster and perform better when features have vastly different ranges.

Mathematically, it is represented by the **Z-Score formula:**: 

$$
Z = \frac{X - \mu}{\sigma}
$$

Where:

- $$X\$$ = original value  
- $$\mu$$ = mean of dataset  
- &sigma;  = standard deviation  

**Key Points:**

- Values roughly lie between **-1 and 1**.  
- Maintains relative differences between values.  
- Improves training for models like **Gradient Descent-based models**.

---

## ğŸ“Š Example Dataset
We will use this dataset:

```python
dataset_1 = [1, 99, 789, 5, 6859, 541, 94142, 7, 50826, 35464]
dataset_0 = [10, 5, 6, 1, 3, 7, 9, 4, 8, 2]  # x-axis
```

### ğŸ“ˆ Step 1 â€” Compute Mean and Standard Deviation
```python
n = len(dataset_1)
mean = sum(dataset_1) / n
variance = sum([(x - mean)**2 for x in dataset_1]) / n
std_dev = variance**0.5

print("Mean:", mean)
print("Standard Deviation:", std_dev)
```
