# Label Encoding in Machine Learning

Label Encoding is a data preprocessing technique in Machine Learning used to convert categorical values into numerical labels. Since most ML algorithms work only with numeric data, categorical features must be encoded before model training. In Label Encoding, each unique category is assigned an integer between 0 and the number of classes.

<img src = "https://media.geeksforgeeks.org/wp-content/uploads/20250724112756132446/categorical_data_encoding_2.webp"/>

The labels are assigned in alphabetical order not based on their position in the dataset which is why encoded values may appear non-sequential when viewed top-down in a DataFrame.

---

## Why It Is Important

Label Encoding is important because many ML algorithms cannot process string values directly making numerical conversion essential for model training.

- Helps algorithms like SVM, Logistic Regression and KNN work with categorical data.
- More memory efficient compared to One Hot Encoding.
- Suitable when categorical features have a natural order or limited unique classes.
- Ensures consistent and compact representation of categories.

---

## Understading Label Encoding

Categorical data is broadly divided into two types:

- **Nominal Data:** Categories without inherent order like colors: red, blue, green.
- **Oridinal Data:** Categories with a natural order like satisfaction levels: low, medium, high.

Label encoding works well for data that has a clear order, like Low, Medium, High, because the numbers correctly show the ranking. But for data without any order, like colors, using numbers can confuse the model. For example, if Red = 0, Blue = 1, and Green = 2, the model might think Green is “bigger” than Blue or Red, which isn’t true. Algorithms like linear regression treat these numbers as real values and assume the differences between them are meaningful, which can create false relationships and lead to wrong predictions. That’s why Label encoding works well for data that has a clear order, like Low, Medium, High, because the numbers correctly show the ranking. But for data without any order, like colors, using numbers can confuse the model. For example, if Red = 0, Blue = 1, and Green = 2, the model might think Green is “bigger” than Blue or Red, which isn’t true. Algorithms like linear regression treat these numbers as real values and assume the differences between them are meaningful, which can create false relationships and lead to wrong predictions. That’s why the choice of encoding must align with the data type and the algorithm used.

---

## When to Use Label Encoding

Label Encoding is best when turning categories into numbers won’t confuse the model. It’s useful in situations like:

- Works well for ordinal features with a natural order.
- Good for models like tree-based algorithms that don’t rely on number ranking.
- Useful when there are many categories, making One-Hot Encoding inefficient.
- Saves memory by avoiding extra dummy columns.
- Ensures consistent mapping for training, validation, and deployment.

---

## LabelEncoder in Scikit-Learn

LabelEncoder is a tool in sklearn.preprocessing that changes target labels (y) into numbers like 0, 1, 2, and so on. It is mainly used for output labels, not for input features. It finds all unique classes, sorts them, and gives each class a number. It can also convert numbers back to the original labels.

OneHotEncoder is used for input features. It converts each category into separate columns with 0s and 1s, so no false order is created.

OrdinalEncoder is also used for input features, but it assigns numbers to categories. It is suitable only when the categories have a clear order

## Key Attribute

classes_: An array containing all unique class labels discovered during fitting.

### 1. Encoding Numeric Labels

- *LabelEncoder()* creates an encoder that converts categories into numeric labels.
- *fit()* learns unique sorted classes and stores them in classes_.
- *transform()* maps labels to integers based on order.
- *inverse_transform()* converts encoded values back to original labels.

```python
from sklearn.preprocessing import LabelEncoder

# Create LabelEncoder object
le = LabelEncoder()

# Fit the encoder
le.fit([1, 2, 2, 6])

# Show learned classes
print("Classes:", list(le.classes_))

# Transform values to numbers
print("Transformed:", list(le.transform([1, 1, 2, 6])))

# Convert numbers back to original values
print("Inverse Transformed:", list(le.inverse_transform([0, 0, 1, 2])))
```
**Output:**
```
Classes: [1, 2, 6]
Transformed: [0, 0, 1, 2]
Inverse Transformed: [1, 1, 2, 6]
```

### 2. Encoding String Labels

LabelEncoder can also encode non numeric labels (strings) as long as they are hashable.

```python
from sklearn.preprocessing import LabelEncoder

# Create LabelEncoder object
le = LabelEncoder()

# Fit the encoder
le.fit(["paris", "paris", "tokyo", "amsterdam"])

# Show learned classes
print("Classes:", list(le.classes_))

# Transform values to numbers
print("Transformed:", list(le.transform(["tokyo", "tokyo", "paris"])))

# Convert numbers back to labels
print("Inverse Transformed:", list(le.inverse_transform([2, 2, 1])))
```
**Output:**
```
Classes: ['amsterdam', 'paris', 'tokyo']
Transformed: [2, 2, 1]
Inverse Transformed: ['tokyo', 'tokyo', 'paris']
```

### Methods in LabelEncoder

**1. `fit(y)`**

Learns all unique labels from the data.

```python
le.fit(["cat", "dog", "cat"])
```
It learns:
```
['cat', 'dog']
```

---

**2. `fit_transform(y)`**

Learns the labels and converts them to numbers in one step.

```python
le.fit_transform(["cat", "dog", "cat"])
```
Output:
```
[0, 1, 0]
```

---

**3. `transform(y)`**

Converts labels to numbers after fitting.

```python
le.transform(["dog", "cat"])
```
Output:
```
[1, 0]
```

---

**4. `inverse_transform(y)`**

Converts numbers back to original labels.

```python
le.inverse_transform([1, 0])
```
Output:
```
['dog', 'cat']
```

---

**5. `get_params(deep=True)`**

Returns encoder settings (mostly empty because LabelEncoder has no major parameters)

```python
le.get_params()
```
Output:
```
{}
```

---

**6. `set_params(params)`**

Used to set parameters (not commonly used for LabelEncoder).

```python
le.set_params()
```

---

**7. `get_metadata_routing()`**

Used internally by sklearn.
You don’t need this for normal ML work.

---

**8. `set_output(transform=None)`**

Controls output format (advanced usage).

```python
le.set_output(transform="pandas")
```
Used when working with pandas pipelines.

----

## Implementing Label Encoding

### 1. Using scikit-learn’s LabelEncoder

- LabelEncoder is used to convert categorical text data into numeric values.
- Each unique category is mapped to an integer from 0 to n-classes.
- Useful when encoding single categorical columns or target labels.
- Stores mapping inside .classes_ so we can retrieve original labels later.

```python
from sklearn.preprocessing import LabelEncoder
import pandas as pd

data = pd.DataFrame({
    'Fruit': ['Apple', 'Banana', 'Orange', 'Apple', 'Orange', 'Banana'],
    'Price': [1.2, 0.5, 0.8, 1.3, 0.9, 0.6]
})
le = LabelEncoder()
data['Fruit_Encoded'] = le.fit_transform(data['Fruit'])
print(data)
print("Category Mapping:", le.classes_)
```
**Output:**
```
     Fruit  Price  Fruit_Encoded
0    Apple    1.2              0
1   Banana    0.5              1
2   Orange    0.8              2
3    Apple    1.3              0
4   Orange    0.9              2
5   Banana    0.6              1
Category Mapping: ['Apple' 'Banana' 'Orange']
```
